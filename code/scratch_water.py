# -*- coding: utf-8 -*-
"""
ç¦äº•ç¸£é˜²ç½ç¶²ç«™ - æ°´ä½ç¾æ³è¡¨å®Œæ•´çˆ¬èŸ² (ç¢ºä¿ç²å–æœ€æ–°è³‡æ–™ç‰ˆæœ¬)

åŠŸèƒ½:
1. é€é nw=1 åƒæ•¸ç¢ºä¿è«‹æ±‚çš„æ˜¯ä¼ºæœå™¨ä¸Šçš„æœ€æ–°è³‡æ–™ã€‚
2. çˆ¬å–æ‰€æœ‰åˆ†é çš„æ°´ä½è³‡æ–™ã€‚
3. è™•ç† Shift_JIS æ—¥æ–‡ç·¨ç¢¼ã€‚
4. æ¸…ç†ä¸¦åˆä½µè³‡æ–™ã€‚
5. å°‡çµæœå„²å­˜ç‚º CSV æª”æ¡ˆã€‚
"""
import requests
import pandas as pd
import time
from datetime import datetime

def scrape_fukui_latest_water_level():
    """
    çˆ¬å–ç¦äº•ç¸£é˜²ç½ç¶²ç«™ç•¶å‰ç™¼å¸ƒçš„æœ€æ–°æ°´ä½è§€æ¸¬ç«™è³‡æ–™ã€‚
    """
    # åŸºæœ¬è¨­å®š
    base_url = "https://sabo.pref.fukui.lg.jp/bousai/servlet/bousaiweb.servletBousaiTableStatus"
    total_pages = 6  # æ ¹æ“šç¶²é é¡¯ç¤ºï¼Œç¸½å…±æœ‰ 6 é è³‡æ–™
    all_data_frames = []

    print("é–‹å§‹çˆ¬å–ç¦äº•ç¸£ã€æœ€æ–°ã€æ°´ä½è³‡æ–™...")
    print(f"ç›®æ¨™ç¶²ç«™: {base_url}")
    print("-" * 30)

    # è¿´åœˆéæ­·æ‰€æœ‰é é¢
    for page_num in range(1, total_pages + 1):
        print(f"æ­£åœ¨æŠ“å–ç¬¬ {page_num} / {total_pages} é çš„æœ€æ–°è³‡æ–™...")

        # çµ„åˆè«‹æ±‚åƒæ•¸
        params = {
            'sv': '3',
            'dk': '2',      # '2' ä»£è¡¨æ°´ä½
            'nw': '1',      # â­ï¸ é—œéµåƒæ•¸ï¼š'1' ä»£è¡¨è«‹æ±‚ "Now" (æœ€æ–°) çš„è³‡æ–™
            'st': '1',      # '1' ä»£è¡¨è­¦å ±é †æ’åº
            'sb': '1',
            'pg': page_num  # 'pg' ä»£è¡¨é ç¢¼
        }

        try:
            # ç™¼é€ GET è«‹æ±‚
            response = requests.get(base_url, params=params, timeout=10)
            response.raise_for_status()

            # è¨­å®šæ­£ç¢ºçš„æ—¥æ–‡ç·¨ç¢¼
            response.encoding = 'shift_jis'

            # è§£æè¡¨æ ¼
            tables_on_page = pd.read_html(response.text, attrs={'class': 'tableStatus'})

            if tables_on_page:
                df = tables_on_page[0]
                df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]
                if not df.empty:
                    df = df.iloc[:-1]
                all_data_frames.append(df)
                print(f"ç¬¬ {page_num} é è³‡æ–™è§£ææˆåŠŸï¼Œå…± {len(df)} ç­†æ•¸æ“šã€‚")
            else:
                print(f"è­¦å‘Šï¼šåœ¨ç¬¬ {page_num} é æ‰¾ä¸åˆ°æŒ‡å®šçš„è³‡æ–™è¡¨æ ¼ã€‚")
            
            time.sleep(1)

        except requests.exceptions.RequestException as e:
            print(f"éŒ¯èª¤ï¼šæŠ“å–ç¬¬ {page_num} é æ™‚ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            break
        except Exception as e:
            print(f"éŒ¯èª¤ï¼šè™•ç†ç¬¬ {page_num} é æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
            break

    print("-" * 30)

    if not all_data_frames:
        print("çˆ¬å–çµæŸï¼Œæœªèƒ½ç²å–ä»»ä½•è³‡æ–™ã€‚")
        return

    print("æ‰€æœ‰é é¢æŠ“å–å®Œç•¢ï¼Œæ­£åœ¨åˆä½µè³‡æ–™...")
    try:
        full_df = pd.concat(all_data_frames, ignore_index=True)
        
        # ç”¢ç”Ÿä¸€å€‹åŒ…å«åŸ·è¡Œæ™‚é–“çš„æª”æ¡ˆåç¨±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = r"/Users/wangweizhi/Desktop/file/Project/Project_Fukui/fukui_summer/dataset/fukui_æ°´ä½.csv"
        
        full_df.to_csv(output_filename, index=False, encoding='utf-8-sig')

        print("\nğŸ‰ ä»»å‹™å®Œæˆï¼ğŸ‰")
        print(f"ç¸½å…±åˆä½µäº† {len(full_df)} ç­†è§€æ¸¬ç«™è³‡æ–™ã€‚")
        print(f"è³‡æ–™å·²æˆåŠŸå„²å­˜è‡³æª”æ¡ˆ: {output_filename}")

    except Exception as e:
        print(f"éŒ¯èª¤ï¼šåˆä½µæˆ–å„²å­˜è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# --- åŸ·è¡Œä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    scrape_fukui_latest_water_level()